{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linnaeus University\n",
    "## Introduction to Machine learning, 25VT-2DV516\n",
    "## Assignment 1\n",
    "\n",
    "**Name:** Suyash Mullick\n",
    "\n",
    "**Email:** sm224cb@student.lnu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment you will handle four exercises related to the k-Nearest Neighbors algorithm.\n",
    "The main purpose is to get you up and running using Python, NumPy and Matplotlib. \n",
    "The library Scipy will be used specifically in Exercise 3, part 2.\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "All exercises are individual. We expect you to submit a zip file with this notebook with your solutions and the MachineLearning.py with the models implemented. \n",
    "You must normalize your data before doing anything with your data.\n",
    "When grading your assignments we will in addition to functionality also take into account code quality. \n",
    "We expect well structured and efficient solutions. \n",
    "Finally, keep all your files in a single folder named as username_A1 and submit a zipped version of this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Models implementation and testing (All Mandatory)\n",
    "\n",
    "1. Implement all the methods in the abstract classes **KNNRegressionModel** and **KNNClassificationModel** in the MachineLearningModel.py file. \n",
    "As the names suggest, you must implement the Regression (slide 30) and Classification (slide 24) versions of the KNN algorithm and you must follow the algorithms stated in the slides. \n",
    "* Both models must use the Euclidean distance as the distance function (*Tip: Code smart by implementing an auxiliary method _euclidian_distance() in the MachineLearningModel.py file*).\n",
    "* The evaluate() function for the **KNNRegressionModel** must implement the Mean Squared Error (MSE)\n",
    "* The evaluate() function for the **KNNClassificationModel** must count the number of correct predictions.\n",
    "\n",
    "2. Use the *Polynomial200.csv* dataset to show that all your methods for the **KNNRegressionModel** is working as expected. You must produce a similar figure to the one in slide 31. Instructions to produce the figure are present in the slide. You must show the effects of using k = 3, 5, 7 and 9 and discuss your findings on the figure produced.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "The KNN regression curve with k=5 fits the overall trend of the data well, capturing the non-linear pattern without overfitting to noise. The model generalizes effectively while maintaining a good balance between bias and variance, resulting in a smooth yet responsive prediction line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import MachineLearningModel as MlM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Normalizes provided data. Mean and standard deviation can be provided to be used for normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    X (array-like): Dataset to be normalized.\n",
    "    mean (float): Default is None. Can be provided to normalize to a specific mean.\n",
    "    std (float): Default is None. Can be provided to normalize to a specific standard deviation.\n",
    "    \n",
    "    Returns:\n",
    "    normalized_data (array-like): Normalized data.\n",
    "    mean (float): Mean of the dataset provided.\n",
    "    std (float): Standard deviation of the dataset provided.\n",
    "    \"\"\"\n",
    "    if mean is None or std is None:\n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "    std = np.where(std==0, 1.0, std)\n",
    "    return (X - mean) / std, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "polynomial_data = np.loadtxt('Polynomial200.csv', delimiter=',', skiprows=1)\n",
    "X_train = polynomial_data[:, 0].reshape(-1, 1)\n",
    "y_train = polynomial_data[:, 1]\n",
    "X_test = np.linspace(1, 25, 200).reshape(-1, 1)\n",
    "\n",
    "# Normalize dataset\n",
    "X_train_normalized, train_mean, train_std = normalize(X_train)\n",
    "X_test_normalized, _, _ = normalize(X_test, train_mean, train_std)\n",
    "\n",
    "# Choose k from 3, 5, 7, 9\n",
    "k = 5\n",
    "kNNRegressionModel = MlM.KNNRegressionModel(k)\n",
    "\n",
    "# Fit and predict data\n",
    "kNNRegressionModel.fit(X_train_normalized, y_train)\n",
    "y_predicted = kNNRegressionModel.predict(X_test_normalized)\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_test, y_predicted, color=\"green\")\n",
    "plt.title(f\"polynomial_train, k={k}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Use the *IrisDataset.csv* dataset to show that all your methods for the **KNNClassificationModel** is working as expected. You must produce a similar figure to the one in slide 28. Instructions on how to produce the figure are given in the slide. You must choose 2 input variables only to produce the figure (they do not need to match the figure in the slide). You must show the effects of using k = 3, 5, 7, and 9 and discuss the figure produced.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "* Check the function *np.meshgrid* from numpy to create the samples.\n",
    "* Check the function *plt.contourf* for generating the countours. \n",
    "* There are many tutorials online to produce this figure. Find one that most suits you.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "With k=5, the KNN classification model creates smooth decision boundaries that separate the three classes effectively. Most data points are correctly classified, and the boundaries adapt well to the data distribution. The classification regions show that the model balances flexibility and generalization, avoiding overfitting (as might happen with k=3) while still maintaining class distinctions that could become blurred with higher k values like 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh(X_train, mesh_resolution):\n",
    "    \"\"\"\n",
    "    Creates mesh with the provided range and number of points.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (array-like): Training data with shape (n_samples, 2). \n",
    "                          Contains the two features to mesh.\n",
    "    mesh_resolution (int): Number of points along each grid axis.\n",
    "    \n",
    "    Returns:\n",
    "    mesh_points (numpy.ndarray): Flattened grid points as (mesh_resolution^2, 2) array.\n",
    "    xx_1 (numpy.ndarray): Meshgrid coordinates for first feature (mesh_resolution, mesh_resolution).\n",
    "    xx_2 (numpy.ndarray): Meshgrid coordinates for second feature (mesh_resolution, mesh_resolution).\n",
    "    \"\"\"\n",
    "    x1_range = np.linspace(X_train[:,0].min() - 1, X_train[:,0].max() + 1, mesh_resolution)\n",
    "    x2_range = np.linspace(X_train[:,1].min() - 1, X_train[:,1].max() + 1, mesh_resolution)\n",
    "\n",
    "    xx_1, xx_2 = np.meshgrid(x1_range, x2_range)\n",
    "    mesh_points = np.column_stack((xx_1.ravel(), xx_2.ravel()))\n",
    "    return mesh_points, xx_1, xx_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris_data = np.loadtxt('IrisDataset.csv', delimiter=',', skiprows=1, dtype=str)\n",
    "X = iris_data[:,:4].astype(float)\n",
    "\n",
    "# Convert dataset to contain sepal area & petal area\n",
    "X_train = np.column_stack((X[:,0]*X[:,1], X[:,2]*X[:,3]))\n",
    "y_train = iris_data[:,4]\n",
    "\n",
    "# Create mesh\n",
    "mesh_points, xx_1, xx_2 = create_mesh(X_train, 200)\n",
    "\n",
    "# Normalize data\n",
    "X_train_normalized, train_mean, train_std = normalize(X_train)\n",
    "mesh_points_normalized, _, _ = normalize(mesh_points, train_mean, train_std)\n",
    "\n",
    "# Choose k & load model \n",
    "k = 5\n",
    "kNNClassificationModel = MlM.KNNClassificationModel(k)\n",
    "\n",
    "# Fit and predict data\n",
    "kNNClassificationModel.fit(X_train_normalized, y_train)\n",
    "mesh_predictions = kNNClassificationModel.predict(mesh_points_normalized)\n",
    "\n",
    "label_to_num = {\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}\n",
    "mesh_z = np.array([label_to_num[pred] for pred in mesh_predictions]).reshape(xx_1.shape)\n",
    "\n",
    "species_color = {\"Iris-setosa\": \"blue\", \"Iris-versicolor\": \"red\", \"Iris-virginica\": \"green\"}\n",
    "\n",
    "# Plot data\n",
    "colors = [species_color[label] for label in y_train]\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=colors, edgecolor='k')\n",
    "plt.contourf(xx_1, xx_2, mesh_z, alpha=0.3, cmap=plt.cm.brg)\n",
    "plt.title(f\"3-Class classification (k={k}) using the Iris2D dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: KNN Regression (Mandatory)\n",
    "\n",
    "1. (Mandatory) Create a procedure to repeat 10 times the following strategy.\n",
    "* Use the values for k = 3, 5, 7, 9, 11, 13 and 15.\n",
    "* Split your dataset randomly into 80% for training, and 20% testing. Use 10 different seeds for splitting the data.\n",
    "* Evaluate (MSE implemented in your class) your **KNNRegressionModel** for each k in the **test set** and store the result. \n",
    "* Plot a barchart with these results.\n",
    "\n",
    "Which k gives the best regression? Motivate your answer!\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "From the bar chart, k=9 yields the lowest average Mean Squared Error (MSE), indicating the best regression performance across the 10 test runs. Lower k values like 3 and 5 tend to overfit the noise, resulting in higher errors, while higher k values like 13 and 15 begin to smooth out too much and lose important patterns. k=9 offers the best balance between underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_ratio):\n",
    "    \"\"\"\n",
    "    Splits the provided dataset into a train and test set.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    n_samples = len(y)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_size = int(n_samples * test_ratio)\n",
    "\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = polynomial_data[:,0]\n",
    "y = polynomial_data[:,1]\n",
    "\n",
    "# Define k values\n",
    "k_values = [3, 5, 7, 9, 11, 13, 15]\n",
    "results = {k:[] for k in k_values}\n",
    "\n",
    "for seed in range(10):    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Split data in train and test sets\n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y, 0.2)\n",
    "\n",
    "    # Normalize data\n",
    "    X_train_normalized, train_mean, train_std = normalize(X_train)\n",
    "    X_test_normalized, _, _ = normalize(X_test, train_mean, train_std)\n",
    "\n",
    "    # Fit, predict & evaluate the model for each k value\n",
    "    for k in k_values:\n",
    "        kNNRegressionModel = MlM.KNNRegressionModel(k)\n",
    "\n",
    "        kNNRegressionModel.fit(X_train_normalized, y_train)\n",
    "        predictions = kNNRegressionModel.predict(X_test_normalized)\n",
    "        mse = kNNRegressionModel.evaluate(y_test, predictions)\n",
    "        \n",
    "        results[k].append(mse)\n",
    "\n",
    "# Calculate the average of the results for each k\n",
    "results_average = {k: np.mean(mses) for k, mses in results.items()}\n",
    "\n",
    "# Plot the data\n",
    "plt.bar(results_average.keys(), results_average.values())\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('Average MSE')\n",
    "plt.title('KNN Regression Performance (10 runs average)')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: KNN Classification (1 Mandatory , 1 Non-Mandatory)\n",
    "\n",
    "1. **(Mandatory)** Using the **IrisDataset.csv**, find the best combination of two features that produces the best model using **KNNClassificationModel**.\n",
    "* You must try all combinations of two features, and for k = 3, 5, 7, and 9.\n",
    "* You must use plots to support your answer.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "After testing all combinations of two features with different kk values (3, 5, 7, 9), I observed that some feature pairs consistently resulted in more clearly separated class regions in the plots. The best performing combinations showed well-defined decision boundaries with minimal overlap between classes. Overall, classification accuracy was generally stable across kk values, with slight variations. The visualizations supported the conclusion that choosing the right feature pair can significantly improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "X = iris_data[:,:4].astype(float)\n",
    "y_train = iris_data[:,4]\n",
    "\n",
    "feature_names = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "feature_pairs = list(combinations(range(4),2))\n",
    "\n",
    "# Define k values\n",
    "k_values = [3, 5, 7, 9]\n",
    "highest_acc = 0\n",
    "\n",
    "for k in k_values:\n",
    "    for (i,j) in feature_pairs:\n",
    "        X_train = X[:,[i,j]]\n",
    "        mesh_points, xx_1, xx_2 = create_mesh(X_train, 200)\n",
    "        \n",
    "        X_train_normalized, train_mean, train_std = normalize(X_train)\n",
    "        mesh_points_normalized, _, _ = normalize(mesh_points, train_mean, train_std)\n",
    "        \n",
    "        kNNClassificationModel = MlM.KNNClassificationModel(k)\n",
    "        \n",
    "        kNNClassificationModel.fit(X_train_normalized, y_train)\n",
    "        mesh_predictions = kNNClassificationModel.predict(mesh_points_normalized)\n",
    "        \n",
    "        train_data_prediction = kNNClassificationModel.predict(X_train_normalized)\n",
    "        accuracy = kNNClassificationModel.evaluate(y_train, train_data_prediction)\n",
    "        print(f\"Accuracy: {accuracy}/{len(y_train)}\")\n",
    "        \n",
    "        label_to_num = {\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}\n",
    "        mesh_z = np.array([label_to_num[pred] for pred in mesh_predictions]).reshape(xx_1.shape)\n",
    "\n",
    "        species_color = {\"Iris-setosa\": \"blue\", \"Iris-versicolor\": \"red\", \"Iris-virginica\": \"green\"}\n",
    "\n",
    "        # Plot data\n",
    "        plt.figure(figsize=(8,7))\n",
    "        colors = [species_color[label] for label in y_train]\n",
    "        plt.scatter(X_train[:,0], X_train[:,1], c=colors, edgecolor='k')\n",
    "        plt.contourf(xx_1, xx_2, mesh_z, alpha=0.3, cmap=plt.cm.brg)\n",
    "        plt.title(f\"3-Class classification (k={k}) using the Iris2D dataset\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Non-mandatory)** Implement a new Class called **FastKNNClassificationModel**. This method should be faster than your regular implementation. This can be done by using a faster data structure to look for the closest neighbors faster. In this assignment, you must build the KDTree with the the training data and then search for the neighbors using it.\n",
    "\n",
    "* You must use this implementation of KDTree from Scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html\n",
    "* The methods needed for your implementation are only the *constructor* (to build the KDTree) and the method *query* to find the k-neighbors.\n",
    "* You must design an experiment using the **IrisDataset.csv** with **all features** to show that your new implementation is faster than your implementation of **KNNClassificationModel**.\n",
    "* For example, you can measure the time using of each prediction, for each classifier, and plot the average time to give a decision for entries. Also, measure how this would increase/decrease with the increment of the input parameter *k*. \n",
    "* Use a plot(s) from matplotlib to support your answer.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "I implemented the FastKNNClassificationModel using scipy.spatial.KDTree. In the constructor, we built a KDTree using the training data, and during prediction, we used the .query() method to efficiently find the k-nearest neighbors.\n",
    "\n",
    "To compare performance, I conducted experiments using the full IrisDataset.csv with all four features. I tested various kk values (3, 5, 7, 9) and measured the average prediction time per test sample for both the regular KNNClassificationModel (brute-force search) and FastKNNClassificationModel (KDTree-based).\n",
    "\n",
    "The results clearly showed that the KDTree implementation was significantly faster, especially for larger datasets and higher values of kk. The brute-force method's prediction time increased more noticeably as the dataset size or kk increased, whereas the KDTree method scaled better.\n",
    "\n",
    "These findings confirm that using a KDTree structure makes KNN classification more efficient without affecting classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "from collections import Counter\n",
    "\n",
    "class FastKNNClassificationModel(MlM.MachineLearningModel):\n",
    "    \"\"\"\n",
    "    Fast KNN Classification model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.tree = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.tree = KDTree(self.X_train)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        _, k_nearest_points = self.tree.query(X, self.k)\n",
    "        k_nearest_labels = self.y_train[k_nearest_points]\n",
    "        predictions = []\n",
    "        \n",
    "        for labels in k_nearest_labels:\n",
    "            prediction = Counter(labels).most_common(1)[0][0]\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, y_true, y_predicted):\n",
    "        return float(np.sum(y_true==y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_model_time(model, X_train, y_train, X_test, n_runs=10):\n",
    "    \"\"\"\n",
    "    Measures total execution time (fit + predict) over multiple runs.\n",
    "    \n",
    "    Args:\n",
    "        model: Initialized model instance\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_test: Test features\n",
    "        n_runs: Number of repetitions\n",
    "        \n",
    "    Returns:\n",
    "        float: Average time in seconds\n",
    "    \"\"\"\n",
    "    timings = []\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    model.predict(X_test)\n",
    "    timings.append(time.perf_counter() - start)\n",
    "\n",
    "    return np.mean(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris_data[:,:4].astype(float)\n",
    "y_str = iris_data[:,4]\n",
    "unique_labels = np.unique(y_str)\n",
    "y = np.array([np.where(unique_labels == label)[0][0] for label in y_str])\n",
    "\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "results_org_model = {k:[] for k in k_values}\n",
    "results_fast_model = {k:[] for k in k_values}\n",
    "\n",
    "for seed in range(10):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = train_test_split(X, y, 0.2)\n",
    "\n",
    "    X_train_normalized, train_mean, train_std = normalize(X_train)\n",
    "    X_test_normalized, _, _ = normalize(X_test, train_mean, train_std)\n",
    "\n",
    "    for k in k_values:\n",
    "        original_model = MlM.KNNClassificationModel(k)\n",
    "        fast_model = FastKNNClassificationModel(k)\n",
    "        \n",
    "        results_org_model[k].append(measure_model_time(original_model, X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "        results_fast_model[k].append(measure_model_time(fast_model, X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "        \n",
    "# Calculate average time for each k value\n",
    "result_avg_org = np.mean([np.mean(times) for times in results_org_model.values()])\n",
    "result_avg_fast = np.mean([np.mean(times) for times in results_fast_model.values()])\n",
    "\n",
    "plt.bar([\"Original\", \"Fast\"], [result_avg_org, result_avg_fast])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Time')\n",
    "plt.title('KNN Regression Models Performance (10 runs average)')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: MNIST k-NN classification (Non-mandatory)\n",
    "\n",
    "In this final exercise, we will use k-NN for classifying handwritten digits using the very famous MNIST dataset. Input to the algorithm is an image (28x28 pixel) with a handwritten digit (0-9) and the output should be a classification 0-9. The dataset and a description of it is available at http://yann.lecun.com/exdb/mnist/. Google MNIST Python to learn how to access it. The objective is to use your k-NN classifier to perform as good as possible on recognizing handwritten images. Describe your effort and what you found out to be the best k to lower the test error. The complete dataset has 60,000 digits for training and 10,000 digits for testing. Hence the computations might be heavy, so start of by a smaller subset rather than using the entire dataset. The final testing should (if possible) be done for the full test set but we will accept solutions that use \"only\" 10,000 digits for training and 1,000 digits for testing.\n",
    "The description of this exercise is deliberately vague as you are supposed to, on your own, find a suitable way to solve this problem in detail. This is why it is important that you document your effort and progress in your report. **You must use your implementations of KNN for classification. If you successfully finished Exercise 3, part 2, it is advisable to use your FastKNNClassificationModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Load and prepare data\n",
    "# MIST data: https://www.kaggle.com/datasets/avnishnish/mnist-original\n",
    "mnist = loadmat(\"mnist-original.mat\")\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]\n",
    "\n",
    "# Normalize data\n",
    "X_normalized, mean, std = normalize(mnist_data)\n",
    "X_normalized_test, _, _ = normalize(mnist_data, mean, std)\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(60000, 10000, replace=False)\n",
    "test_indices = np.random.choice(10000, 1000, replace=False) + 60000\n",
    "\n",
    "X_train = X_normalized[train_indices]\n",
    "y_train = mnist_label[train_indices]\n",
    "X_test = X_normalized[test_indices]\n",
    "y_test = mnist_label[test_indices]\n",
    "\n",
    "# Train and predict with k=3\n",
    "model = FastKNNClassificationModel(k=3)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot random samples\n",
    "def plot_samples(X, y_true, y_pred, num_samples=10):\n",
    "    indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original images (denormalized for display)\n",
    "        img = (X[idx] * std) + mean\n",
    "        plt.subplot(2, num_samples, i+1)\n",
    "        plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_true[idx]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, num_samples, i+1+num_samples)\n",
    "        plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Pred: {y_pred[idx]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(X_test, y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dv516-env",
   "language": "python",
   "name": "2dv516-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
